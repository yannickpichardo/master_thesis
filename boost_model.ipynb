{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost and XGBoost models on the loan-level delinquency rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "asdf\n",
    "asdf\n",
    "a\n",
    "sdf\n",
    "\n",
    "asdf\n",
    "\n",
    "df\n",
    "\n",
    "asdf\n",
    "\n",
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 4 datasets with 4 different MEI projected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort out categorical variables and other non-numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming agg_loan is your dataframe and it's already loaded\n",
    "# Let's say the 'D90' column is your binary target and the rest are features\n",
    "\n",
    "# Splitting the data into features and target\n",
    "X = agg_loan.drop('D90', axis=1)\n",
    "y = agg_loan['D90']\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# AdaBoost does not inherently have a feature selection mechanism like XGBoost,\n",
    "# so to \"force\" a feature like DHRI, you might consider feature engineering techniques\n",
    "# or include it in every set of hyperparameters if it was categorical. Since it's not,\n",
    "# AdaBoost will naturally give it a weight in the model training process.\n",
    "\n",
    "# Create the AdaBoost model\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "# Hyperparameter tuning with cross-validation\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV with the specified parameter grid\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='f1', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = AdaBoostClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC AUC Score: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# Assuming agg_loan_xgb is your dataframe and it's already loaded\n",
    "# Let's say the 'D90' column is your binary target and the rest are features\n",
    "\n",
    "# Splitting the data into features and target\n",
    "X = agg_loan_xgb.drop('D90', axis=1)\n",
    "y = agg_loan_xgb['D90']\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)\n",
    "\n",
    "# Hyperparameter tuning with cross-validation\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 10, 20],\n",
    "}\n",
    "\n",
    "# Including \"DHRI\" in the model; assume that \"DHRI\" is continuous.\n",
    "X_train['DHRI'] = X_train['DHRI'].astype(float)\n",
    "\n",
    "\n",
    "# GridSearchCV with the specified parameter grid\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(\n",
    "    X_train, \n",
    "    y_train  # Call the callback function every 50 rounds\n",
    ")\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC AUC Score: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage file to SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sqlite3\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics.progress import ProgressBar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load necessary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file_layout\n",
    "layout = pd.read_excel(\"../../Data/mortgage_data/file_layout.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column names and data types for both origination and performance datasets\n",
    "orig_layout = layout['Origination Data File']\n",
    "perf_layout = layout['Monthly Performance Data File']\n",
    "\n",
    "# Extract column names and data types\n",
    "orig_column_names = orig_layout['ATTRIBUTE NAME'].tolist()\n",
    "perf_column_names = perf_layout['ATTRIBUTE NAME'].tolist()\n",
    "\n",
    "cols_keep_perf = perf_layout['KEEP'].tolist()\n",
    "cols_keep_orig = orig_layout['KEEP'].tolist()\n",
    "del orig_layout, perf_layout, layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the origination and the performance datasets into dictionary. Also drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets: 100%|██████████| 24/24 [01:34<00:00,  3.94s/it]\n"
     ]
    }
   ],
   "source": [
    "def load_yearly_data(year, base_dir=\"../../Data/mortgage_data\"):\n",
    "    \"\"\"\n",
    "    Load and format the origination and performance datasets for a given year, considering the folder structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - year: The year for which to load the data.\n",
    "    - base_dir: The base directory where the datasets are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - orig_data: Formatted origination dataset for the given year.\n",
    "    - perf_data: Formatted performance dataset for the given year.\n",
    "    \"\"\"\n",
    "    # Construct file paths considering the \"sample_YYYY\" folder structure\n",
    "    orig_file_path = f\"{base_dir}/sample_{year}/sample_orig_{year}.txt\"\n",
    "    perf_file_path = f\"{base_dir}/sample_{year}/sample_svcg_{year}.txt\"\n",
    "    \n",
    "    # Load origination data\n",
    "    orig_data = pd.read_csv(orig_file_path, sep=\"|\", header=None, low_memory=False)\n",
    "    #select only the first 22 columns\n",
    "    orig_data = orig_data.iloc[:, 0:22]\n",
    "    #rename columns according to orig_column_names first 22\n",
    "    orig_data.columns = orig_column_names[0:22]\n",
    "    \n",
    "    # Load performance data\n",
    "    perf_data = pd.read_csv(perf_file_path, sep=\"|\", header=None, names=perf_column_names, low_memory=False)\n",
    "        #function that drops columns where cols_keep is 0\n",
    "    def drop_cols(data, cols_keep, col_names):\n",
    "        cols_to_drop = [col_names[i] for i, val in enumerate(cols_keep) if val == 0]\n",
    "        return data.drop(columns=cols_to_drop)\n",
    "    try:\n",
    "        orig_data = drop_cols(orig_data, cols_keep_orig[0:22], orig_column_names)\n",
    "        perf_data = drop_cols(perf_data, cols_keep_perf, perf_column_names)\n",
    "        # display('cols dropped')\n",
    "    except:\n",
    "        # display('no cols dropped')\n",
    "        pass\n",
    "    orig_data = orig_data.dropna(axis=1, how='all')\n",
    "    perf_data = perf_data.dropna(axis=1, how='all')\n",
    "    return orig_data, perf_data\n",
    "\n",
    "def load_all_datasets(start_year=1999, end_year=2022, base_dir=\"../../Data/mortgage_data/\"):\n",
    "    \"\"\"\n",
    "    Load all origination and performance datasets for a given range of years.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_year: The starting year (inclusive) for which to load the data.\n",
    "    - end_year: The ending year (inclusive) for which to load the data.\n",
    "    - base_dir: The base directory where the datasets are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - datasets: Dictionary containing formatted origination and performance datasets for the given range of years.\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for year in tqdm(range(start_year, end_year + 1), desc = \"Loading datasets\"):\n",
    "        orig_data, perf_data = load_yearly_data(year, base_dir=base_dir)\n",
    "        datasets[f\"orig_{year}\"] = orig_data\n",
    "        datasets[f\"perf_{year}\"] = perf_data\n",
    "    return datasets\n",
    "\n",
    "datasets_tot = load_all_datasets(start_year=1999, end_year=2022)\n",
    "del cols_keep_orig, cols_keep_perf, orig_column_names, perf_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Origination Dataset with Performance Dataset on LSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_datasets(datasets):\n",
    "    \"\"\"\n",
    "    Merge all origination and performance datasets within the provided dictionary according to their year.\n",
    "    \n",
    "    Parameters:\n",
    "    - datasets: Dictionary containing formatted origination and performance datasets.\n",
    "    \n",
    "    Returns:\n",
    "    - merged_datasets: Dictionary containing merged datasets for each year.\n",
    "    \"\"\"\n",
    "    \n",
    "    def merge_orig_with_perf(orig_data, perf_data):\n",
    "        merged_data = pd.merge(perf_data, orig_data, on=\"LSN\", how=\"left\")\n",
    "        return merged_data\n",
    "    \n",
    "    merged_datasets = {}\n",
    "    # Extract the range of years from the dataset keys\n",
    "    years = sorted(set(int(key.split(\"_\")[-1]) for key in datasets.keys()))\n",
    "    for year in tqdm(years, desc=\"Merging datasets\"):\n",
    "        orig_key = f\"orig_{year}\"\n",
    "        perf_key = f\"perf_{year}\"\n",
    "        if orig_key in datasets and perf_key in datasets:\n",
    "            merged_data = merge_orig_with_perf(datasets[orig_key], datasets[perf_key])\n",
    "            merged_data['Date'] = pd.to_datetime(merged_data['MRP'], format = '%Y%m')\n",
    "            merged_data = merged_data.drop(['MRP'], axis=1)\n",
    "            merged_data = merged_data[[\"Date\"] + [\"LSN\"] + [col for col in merged_data.columns if col != \"LSN\" and col != \"Date\"]]\n",
    "            merged_datasets[f\"fm_{year}\"] = merged_data\n",
    "            print(\"merged\", year)\n",
    "    return merged_datasets\n",
    "\n",
    "# Merge all datasets in the provided dictionary (in this case, datasets_demo)\n",
    "merged_datasets = merge_all_datasets(datasets_tot)\n",
    "merged_datasets.keys()  # Display the keys of the merged datasets dictionary\n",
    "del datasets_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 34.74 s\n",
      "fm_1999 added and dropped\n",
      "[########################################] | 100% Completed | 33.09 s\n",
      "fm_2000 added and dropped\n",
      "[########################################] | 100% Completed | 33.50 s\n",
      "fm_2001 added and dropped\n",
      "[########################################] | 100% Completed | 34.57 s\n",
      "fm_2002 added and dropped\n",
      "[########################################] | 100% Completed | 39.04 s\n",
      "fm_2003 added and dropped\n",
      "[########################################] | 100% Completed | 39.77 s\n",
      "fm_2004 added and dropped\n",
      "[########################################] | 100% Completed | 41.34 s\n",
      "fm_2005 added and dropped\n",
      "[########################################] | 100% Completed | 41.33 s\n",
      "fm_2006 added and dropped\n",
      "[########################################] | 100% Completed | 40.78 s\n",
      "fm_2007 added and dropped\n",
      "[########################################] | 100% Completed | 36.98 s\n",
      "fm_2008 added and dropped\n",
      "[########################################] | 100% Completed | 35.88 s\n",
      "fm_2009 added and dropped\n",
      "[########################################] | 100% Completed | 37.35 s\n",
      "fm_2010 added and dropped\n",
      "[########################################] | 100% Completed | 36.27 s\n",
      "fm_2011 added and dropped\n",
      "[########################################] | 100% Completed | 37.04 s\n",
      "fm_2012 added and dropped\n",
      "[########################################] | 100% Completed | 35.77 s\n",
      "fm_2013 added and dropped\n",
      "[########################################] | 100% Completed | 34.15 s\n",
      "fm_2014 added and dropped\n",
      "[########################################] | 100% Completed | 35.85 s\n",
      "fm_2015 added and dropped\n",
      "[########################################] | 100% Completed | 35.55 s\n",
      "fm_2016 added and dropped\n",
      "[########################################] | 100% Completed | 33.28 s\n",
      "fm_2017 added and dropped\n",
      "[########################################] | 100% Completed | 33.30 s\n",
      "fm_2018 added and dropped\n",
      "[########################################] | 100% Completed | 32.16 s\n",
      "fm_2019 added and dropped\n",
      "[########################################] | 100% Completed | 31.14 ss\n",
      "fm_2020 added and dropped\n",
      "[########################################] | 100% Completed | 30.15 ss\n",
      "fm_2021 added and dropped\n",
      "[########################################] | 100% Completed | 28.71 ss\n",
      "fm_2022 added and dropped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>3ZIP</th>\n",
       "      <th>LSN</th>\n",
       "      <th>CLDS</th>\n",
       "      <th>CIR</th>\n",
       "      <th>ELTV</th>\n",
       "      <th>DDD</th>\n",
       "      <th>CS</th>\n",
       "      <th>FPD</th>\n",
       "      <th>FIRST_F</th>\n",
       "      <th>MD</th>\n",
       "      <th>CLTV</th>\n",
       "      <th>DTI</th>\n",
       "      <th>LTV</th>\n",
       "      <th>OIR</th>\n",
       "      <th>P_TYPE</th>\n",
       "      <th>OLT</th>\n",
       "      <th>D90</th>\n",
       "      <th>D180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date 3ZIP           LSN  CLDS    CIR  ELTV  DDD   CS     FPD  FIRST_F   \n",
       "0 2022-02-01  125  F22Q10000012     0  2.625    57    0  768  202203        0  \\\n",
       "1 2022-03-01  125  F22Q10000012     0  2.625    48    0  768  202203        0   \n",
       "2 2022-04-01  125  F22Q10000012     0  2.625    52    0  768  202203        0   \n",
       "3 2022-05-01  125  F22Q10000012     0  2.625    40    0  768  202203        0   \n",
       "4 2022-06-01  125  F22Q10000012     0  2.625    39    0  768  202203        0   \n",
       "\n",
       "       MD  CLTV  DTI  LTV    OIR P_TYPE  OLT  D90  D180  \n",
       "0  203702    57   28   57  2.625     SF  180    0     0  \n",
       "1  203702    57   28   57  2.625     SF  180    0     0  \n",
       "2  203702    57   28   57  2.625     SF  180    0     0  \n",
       "3  203702    57   28   57  2.625     SF  180    0     0  \n",
       "4  203702    57   28   57  2.625     SF  180    0     0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_dataset(df):\n",
    "    # Transformations\n",
    "    df['3ZIP'] = df['POSTAL'].astype(str).str[:3]\n",
    "    df['DDD'] = df['DDD'].fillna(0).replace('Y', 1)\n",
    "    df['FIRST_F'] = df['FIRST_F'].replace({'N': 0, 'Y': 1})\n",
    "    #change RA values to 99 in CLDS\n",
    "    df['CLDS'] = df['CLDS'].replace('RA', 99)\n",
    "    df['CLDS'] = df['CLDS'].astype('int16')\n",
    "    df['D90'] = 0\n",
    "    df['D180'] = 0\n",
    "    # Drop columns\n",
    "    df = df.drop(['POSTAL'], axis=1)\n",
    "    \n",
    "    # Process group\n",
    "    def process_group(group):\n",
    "        for val, offset, column in [(3, 3, 'D90'), (7, 6, 'D180')]:\n",
    "            if val in group['CLDS'].values:\n",
    "                delinquency_date = group[group['CLDS'] == val]['Date'].min()\n",
    "                back_date = delinquency_date - pd.DateOffset(months=offset)\n",
    "                group.loc[group['Date'] == back_date, column] = 1\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby('LSN').apply(process_group, meta=df._meta)\n",
    "    #ungroup df\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Move Date and 3ZIP to the front\n",
    "    df = df[[\"Date\", \"3ZIP\"] + [col for col in df.columns if col not in [\"Date\", \"3ZIP\"]]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert to Dask DataFrames and process\n",
    "with ProgressBar():\n",
    "    for key in merged_datasets.keys():\n",
    "        print(f\"{key} processing...\")\n",
    "        ddf = dd.from_pandas(merged_datasets[key], npartitions=6)\n",
    "        merged_datasets[key] = process_dataset(ddf).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D90\n",
       "0    3835398\n",
       "1       5487\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_datasets['fm_2005']['D90'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dictionary to accessible pickle file\n",
    "with open(\"../Data/mortgage_data/fm_datasets.pickle\", \"wb\") as f:\n",
    "    pickle.dump(merged_datasets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open connection SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"../../Database/thesis_database.db\"\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store mortgage dataset into SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dataset in merged_datasets.items():\n",
    "    print(\"Writing\", key, \"to database...\")\n",
    "    dataset.to_sql(key, conn, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Closed SQL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

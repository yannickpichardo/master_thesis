{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage file to SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sqlite3\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics.progress import ProgressBar\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load necessary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file_layout\n",
    "layout = pd.read_excel(\"../../Data/mortgage_data/file_layout.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract column names and data types for both origination and performance datasets\n",
    "# orig_layout = layout['Origination Data File']\n",
    "# perf_layout = layout['Monthly Performance Data File']\n",
    "\n",
    "# # Extract column names and data types\n",
    "# orig_column_names = orig_layout['ATTRIBUTE NAME'].tolist()\n",
    "# perf_column_names = perf_layout['ATTRIBUTE NAME'].tolist()\n",
    "\n",
    "# orig_column_force_types = orig_layout['DATA TYPE'].tolist()\n",
    "\n",
    "# #create dict to force data types only if force type is not None\n",
    "# orig_column_force_types = dict(zip(orig_column_names, orig_column_force_types))\n",
    "# orig_column_force_types = {k: v for k, v in orig_column_force_types.items() if v != 0}\n",
    "\n",
    "# cols_keep_perf = perf_layout['KEEP'].tolist()\n",
    "# cols_keep_orig = orig_layout['KEEP'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the origination and the performance datasets into dictionary. Also drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets: 100%|██████████| 24/24 [01:31<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": [
    "def load_yearly_data(year, base_dir=\"../../Data/mortgage_data\", layout = layout):\n",
    "    \"\"\"\n",
    "    Load and format the origination and performance datasets for a given year, considering the folder structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - year: The year for which to load the data.\n",
    "    - base_dir: The base directory where the datasets are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - orig_data: Formatted origination dataset for the given year.\n",
    "    - perf_data: Formatted performance dataset for the given year.\n",
    "    \"\"\"\n",
    "    # Extract column names and data types for both origination and performance datasets\n",
    "    orig_layout = layout['Origination Data File']\n",
    "    perf_layout = layout['Monthly Performance Data File']\n",
    "\n",
    "    # Extract column names and data types\n",
    "    orig_column_names = orig_layout['ATTRIBUTE NAME'].tolist()\n",
    "    perf_column_names = perf_layout['ATTRIBUTE NAME'].tolist()\n",
    "\n",
    "    orig_column_force_types = orig_layout['DATA TYPE'].tolist()\n",
    "    \n",
    "    #create dict to force data types only if force type is not None\n",
    "    orig_column_force_types = dict(zip(orig_column_names, orig_column_force_types))\n",
    "    orig_column_force_types = {k: v for k, v in orig_column_force_types.items() if v != 0}\n",
    "    # display(orig_column_force_types)\n",
    "    cols_keep_perf = perf_layout['KEEP'].tolist()\n",
    "    cols_keep_orig = orig_layout['KEEP'].tolist()\n",
    "    del orig_layout, perf_layout, layout\n",
    "    \n",
    "    # Construct file paths considering the \"sample_YYYY\" folder structure\n",
    "    orig_file_path = f\"{base_dir}/sample_{year}/sample_orig_{year}.txt\"\n",
    "    perf_file_path = f\"{base_dir}/sample_{year}/sample_svcg_{year}.txt\"\n",
    "    \n",
    "\n",
    "    # Load origination data, use orig_column_force_types to force data types\n",
    "    orig_data = pd.read_csv(orig_file_path, sep=\"|\", header=None, low_memory=False, names = orig_column_names, dtype= orig_column_force_types)\n",
    "    #select only the first 22 columns\n",
    "    orig_data = orig_data.iloc[:, 0:22]\n",
    "    # Load performance data\n",
    "    perf_data = pd.read_csv(perf_file_path, sep=\"|\", header=None, names=perf_column_names, low_memory=False)\n",
    "        #function that drops columns where cols_keep is 0\n",
    "    def drop_cols(data, cols_keep, col_names):\n",
    "        cols_to_drop = [col_names[i] for i, val in enumerate(cols_keep) if val == 0]\n",
    "        return data.drop(columns=cols_to_drop)\n",
    "    try:\n",
    "        orig_data = drop_cols(orig_data, cols_keep_orig[0:22], orig_column_names)\n",
    "        perf_data = drop_cols(perf_data, cols_keep_perf, perf_column_names)\n",
    "        # display('cols dropped')\n",
    "    except:\n",
    "        # display('no cols dropped')\n",
    "        pass\n",
    "    orig_data = orig_data.dropna(axis=1, how='all')\n",
    "    perf_data = perf_data.dropna(axis=1, how='all')\n",
    "    return orig_data, perf_data\n",
    "\n",
    "def load_all_datasets(start_year=1999, end_year=2022, base_dir=\"../../Data/mortgage_data/\"):\n",
    "    \"\"\"\n",
    "    Load all origination and performance datasets for a given range of years.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_year: The starting year (inclusive) for which to load the data.\n",
    "    - end_year: The ending year (inclusive) for which to load the data.\n",
    "    - base_dir: The base directory where the datasets are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - datasets: Dictionary containing formatted origination and performance datasets for the given range of years.\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for year in tqdm(range(start_year, end_year + 1), desc = \"Loading datasets\"):\n",
    "        orig_data, perf_data = load_yearly_data(year, base_dir=base_dir)\n",
    "        datasets[f\"orig_{year}\"] = orig_data\n",
    "        datasets[f\"perf_{year}\"] = perf_data\n",
    "    return datasets\n",
    "\n",
    "datasets_tot = load_all_datasets(start_year=1999, end_year=2022)\n",
    "# del cols_keep_orig, cols_keep_perf, orig_column_names, perf_column_names\n",
    "#Takes 1:30 to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Origination Dataset with Performance Dataset on LSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging datasets: 100%|██████████| 24/24 [00:22<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def merge_all_datasets(datasets):\n",
    "    \"\"\"\n",
    "    Merge all origination and performance datasets within the provided dictionary according to their year.\n",
    "    \n",
    "    Parameters:\n",
    "    - datasets: Dictionary containing formatted origination and performance datasets.\n",
    "    \n",
    "    Returns:\n",
    "    - merged_datasets: Dictionary containing merged datasets for each year.\n",
    "    \"\"\"\n",
    "    \n",
    "    def merge_orig_with_perf(orig_data, perf_data):\n",
    "        merged_data = pd.merge(perf_data, orig_data, on=\"LSN\", how=\"left\")\n",
    "        return merged_data\n",
    "    \n",
    "    merged_datasets = {}\n",
    "    # Extract the range of years from the dataset keys\n",
    "    years = sorted(set(int(key.split(\"_\")[-1]) for key in datasets.keys()))\n",
    "    for year in tqdm(years, desc=\"Merging datasets\"):\n",
    "        orig_key = f\"orig_{year}\"\n",
    "        perf_key = f\"perf_{year}\"\n",
    "        if orig_key in datasets and perf_key in datasets:\n",
    "            merged_data = merge_orig_with_perf(datasets[orig_key], datasets[perf_key])\n",
    "            merged_data['Date'] = pd.to_datetime(merged_data['MRP'], format = '%Y%m')\n",
    "            merged_data = merged_data.drop(['MRP'], axis=1)\n",
    "            merged_data = merged_data[[\"LSN\"] + [\"Date\"] + [col for col in merged_data.columns if col != \"LSN\" and col != \"Date\"]]\n",
    "            merged_datasets[f\"fm_{year}\"] = merged_data\n",
    "            # print(\"merged\", year)\n",
    "    return merged_datasets\n",
    "\n",
    "# Merge all datasets in the provided dictionary (in this case, datasets_demo)\n",
    "merged_datasets = merge_all_datasets(datasets_tot)\n",
    "merged_datasets.keys()  # Display the keys of the merged datasets dictionary\n",
    "del datasets_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        datetime64[ns]\n",
       "MSA        string[pyarrow]\n",
       "LSN        string[pyarrow]\n",
       "CLDS       string[pyarrow]\n",
       "AGE                  int64\n",
       "CIR                float64\n",
       "ELTV               float64\n",
       "DDD        string[pyarrow]\n",
       "CS                   int64\n",
       "FPD                  int64\n",
       "FIRST_F    string[pyarrow]\n",
       "MD                   int64\n",
       "MIP                  int64\n",
       "CLTV                 int64\n",
       "DTI                  int64\n",
       "LTV                  int64\n",
       "OIR                float64\n",
       "P_TYPE     string[pyarrow]\n",
       "POSTAL     string[pyarrow]\n",
       "OLT                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_datasets['fm_1999'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fm_1999 processing...\n",
      "[########################################] | 100% Completed | 592.77 ms\n",
      "fm_2000 processing...\n",
      "[########################################] | 100% Completed | 401.58 ms\n",
      "fm_2001 processing...\n",
      "[########################################] | 100% Completed | 518.70 ms\n",
      "fm_2002 processing...\n",
      "[########################################] | 100% Completed | 540.80 ms\n",
      "fm_2003 processing...\n",
      "[########################################] | 100% Completed | 938.16 ms\n",
      "fm_2004 processing...\n",
      "[########################################] | 100% Completed | 889.17 ms\n",
      "fm_2005 processing...\n",
      "[########################################] | 100% Completed | 829.25 ms\n",
      "fm_2006 processing...\n",
      "[########################################] | 100% Completed | 730.75 ms\n",
      "fm_2007 processing...\n",
      "[########################################] | 100% Completed | 603.96 ms\n",
      "fm_2008 processing...\n",
      "[########################################] | 100% Completed | 594.68 ms\n",
      "fm_2009 processing...\n",
      "[########################################] | 100% Completed | 630.36 ms\n",
      "fm_2010 processing...\n",
      "[########################################] | 100% Completed | 776.46 ms\n",
      "fm_2011 processing...\n",
      "[########################################] | 100% Completed | 781.85 ms\n",
      "fm_2012 processing...\n",
      "[########################################] | 100% Completed | 947.08 ms\n",
      "fm_2013 processing...\n",
      "[########################################] | 100% Completed | 880.58 ms\n",
      "fm_2014 processing...\n",
      "[########################################] | 100% Completed | 740.80 ms\n",
      "fm_2015 processing...\n",
      "[########################################] | 100% Completed | 647.43 ms\n",
      "fm_2016 processing...\n",
      "[########################################] | 100% Completed | 693.93 ms\n",
      "fm_2017 processing...\n",
      "[########################################] | 100% Completed | 488.54 ms\n",
      "fm_2018 processing...\n",
      "[########################################] | 100% Completed | 429.74 ms\n",
      "fm_2019 processing...\n",
      "[########################################] | 100% Completed | 381.59 ms\n",
      "fm_2020 processing...\n",
      "[########################################] | 100% Completed | 348.38 ms\n",
      "fm_2021 processing...\n",
      "[########################################] | 100% Completed | 286.58 ms\n",
      "fm_2022 processing...\n",
      "[########################################] | 100% Completed | 105.81 ms\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(df):\n",
    "    # Transformations\n",
    "    # df['3ZIP'] = df['POSTAL'].astype(str).str[:3].astype(str)\n",
    "    # df['STATE'] = df['POSTAL'].apply(postal_to_state)\n",
    "    # df['DDD'] = df['DDD'].fillna(0).replace('Y', 1)\n",
    "    # df['FIRST_F'] = df['FIRST_F'].replace({'N': 0, 'Y': 1})\n",
    "    #change RA values to 99 in CLDS\n",
    "    # df['CLDS'] = df['CLDS'].replace('RA', 99)\n",
    "    # df['CLDS'] = df['CLDS'].astype('int64')\n",
    "    # df['D90'] = 0\n",
    "    # df['D180'] = 0\n",
    "    # # Process group\n",
    "    # def process_group(group):\n",
    "    #     for val, offset, column in [(3, 0, 'D90'), (7, 0, 'D180')]:\n",
    "    #         if val in group['CLDS'].values:\n",
    "    #             delinquency_date = group[group['CLDS'] == val]['Date'].min()\n",
    "    #             back_date = delinquency_date - pd.DateOffset(months=offset)\n",
    "    #             group.loc[group['Date'] == back_date, column] = 1\n",
    "    #     return group\n",
    "    \n",
    "    # df = df.groupby('LSN').apply(process_group, meta=df._meta)\n",
    "    # #ungroup df\n",
    "    # df = df.reset_index(drop=True)\n",
    "    # Move Date and 3ZIP to the front\n",
    "    df = df[[\"Date\", \"MSA\"] + [col for col in df.columns if col not in [\"Date\", \"MSA\"]]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert to Dask DataFrames and process\n",
    "with ProgressBar():\n",
    "    for key in merged_datasets.keys():\n",
    "        print(f\"{key} processing...\")\n",
    "        ddf = dd.from_pandas(merged_datasets[key], npartitions=6)\n",
    "        merged_datasets[key] = process_dataset(ddf).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fm_1999 saved\n",
      "fm_2000 saved\n",
      "fm_2001 saved\n",
      "fm_2002 saved\n",
      "fm_2003 saved\n",
      "fm_2004 saved\n",
      "fm_2005 saved\n",
      "fm_2006 saved\n",
      "fm_2007 saved\n",
      "fm_2008 saved\n",
      "fm_2009 saved\n",
      "fm_2010 saved\n",
      "fm_2011 saved\n",
      "fm_2012 saved\n",
      "fm_2013 saved\n",
      "fm_2014 saved\n",
      "fm_2015 saved\n",
      "fm_2016 saved\n",
      "fm_2017 saved\n",
      "fm_2018 saved\n",
      "fm_2019 saved\n",
      "fm_2020 saved\n",
      "fm_2021 saved\n",
      "fm_2022 saved\n"
     ]
    }
   ],
   "source": [
    "#save merged datasets to pickle one by one\n",
    "for key in merged_datasets.keys():\n",
    "    merged_datasets[key].to_pickle(f\"../../Data/mortgage_data/processed/{key}.pkl\")\n",
    "    print(f\"{key} saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open connection SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"../../Database/thesis_database.db\"\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store mortgage dataset into SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fm_1999 to database...\n",
      "Writing fm_2000 to database...\n",
      "Writing fm_2001 to database...\n",
      "Writing fm_2002 to database...\n",
      "Writing fm_2003 to database...\n",
      "Writing fm_2004 to database...\n",
      "Writing fm_2005 to database...\n",
      "Writing fm_2006 to database...\n",
      "Writing fm_2007 to database...\n",
      "Writing fm_2008 to database...\n",
      "Writing fm_2009 to database...\n",
      "Writing fm_2010 to database...\n",
      "Writing fm_2011 to database...\n",
      "Writing fm_2012 to database...\n",
      "Writing fm_2013 to database...\n",
      "Writing fm_2014 to database...\n",
      "Writing fm_2015 to database...\n",
      "Writing fm_2016 to database...\n",
      "Writing fm_2017 to database...\n",
      "Writing fm_2018 to database...\n",
      "Writing fm_2019 to database...\n",
      "Writing fm_2020 to database...\n",
      "Writing fm_2021 to database...\n",
      "Writing fm_2022 to database...\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in merged_datasets.items():\n",
    "    print(\"Writing\", key, \"to database...\")\n",
    "    dataset.to_sql(key, conn, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Closed SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"../../Database/thesis_database.db\"\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query   =   \"\"\"\n",
    "SELECT *\n",
    "FROM fm_2005\n",
    "\"\"\"\n",
    "fm_2005 = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49994\n",
      "8007\n",
      "0.16015921910629274\n"
     ]
    }
   ],
   "source": [
    "#show number of unique loans\n",
    "print(fm_2005['LSN'].nunique())\n",
    "#show number of loans that do not have MSA (NaN)\n",
    "print(fm_2005[fm_2005['MSA'].isna()]['LSN'].nunique())\n",
    "\n",
    "#show percentage of loans which does not have MSA\n",
    "print(fm_2005[fm_2005['MSA'].isna()]['LSN'].nunique()/fm_2005['LSN'].nunique())\n",
    "fm_2005_msa = fm_2005[fm_2005['MSA'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mainlang_usa_gdf_msa.pkl\n",
    "with open('../../Data/mainland_usa_gdf_msa.pkl', 'rb') as f:\n",
    "    mainland_usa_gdf_msa = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741\n"
     ]
    }
   ],
   "source": [
    "print(mainland_usa_gdf_msa['MSA'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'MSA' and aggregate\n",
    "def custom_agg(series):\n",
    "    if np.issubdtype(series.dtype, np.number):\n",
    "        return series.mean()\n",
    "    else:\n",
    "        return series.iloc[0]\n",
    "\n",
    "agg_funcs = {col: custom_agg for col in mainland_usa_gdf_msa.columns if col != 'MSA'}\n",
    "\n",
    "cols_to_drop = ['HRCN_HLRR', 'HRCN_RISKR', 'HRCN_EALR', 'HRCN_RISK_CATEGORY_QUANTILE', 'HRCN_EALS_Norm']\n",
    "mainland_usa_gdf_msa = mainland_usa_gdf_msa.drop(columns=cols_to_drop)\n",
    "\n",
    "mainland_usa_gdf_msa_aggregated = mainland_usa_gdf_msa.groupby('MSA').mean(agg_funcs).reset_index()\n",
    "# Step 2: Merge the DataFrames\n",
    "fm_2005_msa_hrcn_2 = fm_2005_msa.merge(mainland_usa_gdf_msa_aggregated, on='MSA', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/pickle/mortgage_data/fm_datasets.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yannickpichardo/Desktop/Thesis_repo/master_thesis/to_sql/mortgage_sql.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yannickpichardo/Desktop/Thesis_repo/master_thesis/to_sql/mortgage_sql.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Save dictionary to accessible pickle file\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yannickpichardo/Desktop/Thesis_repo/master_thesis/to_sql/mortgage_sql.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m../../Data/pickle/mortgage_data/fm_datasets.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yannickpichardo/Desktop/Thesis_repo/master_thesis/to_sql/mortgage_sql.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(merged_datasets, f)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Data/pickle/mortgage_data/fm_datasets.pickle'"
     ]
    }
   ],
   "source": [
    "#Save dictionary to accessible pickle file\n",
    "with open(\"../../Data/pickle/mortgage_data/fm_datasets.pickle\", \"wb\") as f:\n",
    "    pickle.dump(merged_datasets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open connection SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"../../Database/thesis_database.db\"\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store mortgage dataset into SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fm_1999 to database...\n",
      "Writing fm_2000 to database...\n",
      "Writing fm_2001 to database...\n",
      "Writing fm_2002 to database...\n",
      "Writing fm_2003 to database...\n",
      "Writing fm_2004 to database...\n",
      "Writing fm_2005 to database...\n",
      "Writing fm_2006 to database...\n",
      "Writing fm_2007 to database...\n",
      "Writing fm_2008 to database...\n",
      "Writing fm_2009 to database...\n",
      "Writing fm_2010 to database...\n",
      "Writing fm_2011 to database...\n",
      "Writing fm_2012 to database...\n",
      "Writing fm_2013 to database...\n",
      "Writing fm_2014 to database...\n",
      "Writing fm_2015 to database...\n",
      "Writing fm_2016 to database...\n",
      "Writing fm_2017 to database...\n",
      "Writing fm_2018 to database...\n",
      "Writing fm_2019 to database...\n",
      "Writing fm_2020 to database...\n",
      "Writing fm_2021 to database...\n",
      "Writing fm_2022 to database...\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in merged_datasets.items():\n",
    "    print(\"Writing\", key, \"to database...\")\n",
    "    dataset.to_sql(key, conn, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Closed SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618876\n",
      "0.2105044752218223\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#load fm_2007\n",
    "fm_2007 = pd.read_pickle(\"../../Data/mortgage_data/processed/fm_2007.pkl\")\n",
    "#Count MSA NaN\n",
    "print(fm_2007['MSA'].isna().sum())\n",
    "#Percentage of MSA NaN\n",
    "print(fm_2007['MSA'].isna().sum()/len(fm_2007))\n",
    "#Remove MSA NaN\n",
    "fm_2007 = fm_2007.dropna(subset=['MSA'])\n",
    "\n",
    "#extract MSA and POSTAL from fm_2007\n",
    "msa_2007 = fm_2007[['MSA', 'POSTAL']]\n",
    "msa_2007['MSA'] = msa_2007['MSA'].astype(str)\n",
    "\n",
    "#Output true if there are MSA of length 4\n",
    "msa_2007['MSA_4'] = msa_2007['MSA'].str.len() == 5\n",
    "#print number of MSA of length 4\n",
    "print(msa_2007['MSA_4'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file layout\n",
    "layout = pd.read_excel(\"../Data/mortgage_data/file_layout.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column name extraction from Freddie Mac documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheet_names = layout.keys()\n",
    "# Extract column names and data types for both origination and performance datasets\n",
    "orig_layout = layout['Origination Data File']\n",
    "perf_layout = layout['Monthly Performance Data File']\n",
    "\n",
    "# Extract column names and data types\n",
    "orig_column_names = orig_layout['ATTRIBUTE NAME'].tolist()\n",
    "orig_data_types = orig_layout['DATA TYPE & FORMAT'].tolist()\n",
    "perf_column_names = perf_layout['ATTRIBUTE NAME'].tolist()\n",
    "perf_data_types = perf_layout['DATA TYPE & FORMAT'].tolist()\n",
    "\n",
    "cols_keep_perf = perf_layout['KEEP'].tolist()\n",
    "cols_keep_orig = orig_layout['KEEP'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols_and_NAN(data):\n",
    "    #first drop all columns that only have NaN values\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    #drop cols_to_drop\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that drops columns where cols_keep is 0\n",
    "def drop_cols(data, cols_keep, col_names):\n",
    "    cols_to_drop = []\n",
    "    for i in range(len(cols_keep)):\n",
    "        if cols_keep[i] == 0:\n",
    "            cols_to_drop.append(col_names[i])\n",
    "    data = data.drop(cols_to_drop, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yearly_data(year, base_dir=\"../Data/mortgage_data\"):\n",
    "    \"\"\"\n",
    "    Load and format the origination and performance datasets for a given year, considering the folder structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - year: The year for which to load the data.\n",
    "    - base_dir: The base directory where the datasets are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - orig_data: Formatted origination dataset for the given year.\n",
    "    - perf_data: Formatted performance dataset for the given year.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Construct file paths considering the \"sample_YYYY\" folder structure\n",
    "    orig_file_path = f\"{base_dir}/sample_{year}/sample_orig_{year}.txt\"\n",
    "    perf_file_path = f\"{base_dir}/sample_{year}/sample_svcg_{year}.txt\"\n",
    "    \n",
    "    # Load origination data\n",
    "    orig_data = pd.read_csv(orig_file_path, sep=\"|\", header=None, low_memory=False)\n",
    "    #select only the first 22 columns\n",
    "    orig_data = orig_data.iloc[:, 0:22]\n",
    "    #rename columns according to orig_column_names first 22\n",
    "    orig_data.columns = orig_column_names[0:22]\n",
    "    \n",
    "    # Load performance data\n",
    "    perf_data = pd.read_csv(perf_file_path, sep=\"|\", header=None, names=perf_column_names, low_memory=False)\n",
    "    \n",
    "    try:\n",
    "        orig_data = drop_cols(orig_data, cols_keep_orig[0:22], orig_column_names)\n",
    "        perf_data = drop_cols(perf_data, cols_keep_perf, perf_column_names)\n",
    "        # display('cols dropped')\n",
    "    except:\n",
    "        # display('no cols dropped')\n",
    "        pass\n",
    "    orig_data = drop_cols_and_NAN(orig_data)\n",
    "    perf_data = drop_cols_and_NAN(perf_data)\n",
    "    return orig_data, perf_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all data at once into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2003"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2011"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2015"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2020"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_1999', 'perf_1999', 'orig_2000', 'perf_2000', 'orig_2001', 'perf_2001', 'orig_2002', 'perf_2002', 'orig_2003', 'perf_2003', 'orig_2004', 'perf_2004', 'orig_2005', 'perf_2005', 'orig_2006', 'perf_2006', 'orig_2007', 'perf_2007', 'orig_2008', 'perf_2008', 'orig_2009', 'perf_2009', 'orig_2010', 'perf_2010', 'orig_2011', 'perf_2011', 'orig_2012', 'perf_2012', 'orig_2013', 'perf_2013', 'orig_2014', 'perf_2014', 'orig_2015', 'perf_2015', 'orig_2016', 'perf_2016', 'orig_2017', 'perf_2017', 'orig_2018', 'perf_2018', 'orig_2019', 'perf_2019', 'orig_2020', 'perf_2020', 'orig_2021', 'perf_2021', 'orig_2022', 'perf_2022'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_all_datasets(start_year=1999, end_year=2022, base_dir=\"../Data/mortgage_data/\"):\n",
    "    \"\"\"\n",
    "    Load all origination and performance datasets for a given range of years.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_year: The starting year (inclusive) for which to load the data.\n",
    "    - end_year: The ending year (inclusive) for which to load the data.\n",
    "    - base_dir: The base directory where the datasets are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - datasets: Dictionary containing formatted origination and performance datasets for the given range of years.\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        display(year)\n",
    "        orig_data, perf_data = load_yearly_data(year, base_dir=base_dir)\n",
    "        datasets[f\"orig_{year}\"] = orig_data\n",
    "        # display(orig_data.shape)\n",
    "        datasets[f\"perf_{year}\"] = perf_data\n",
    "        # display(perf_data.shape)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# For demonstration purposes, we'll load only the 2022 sample data\n",
    "# To load all years' data, you would simply call load_all_datasets() without the year range\n",
    "datasets_tot = load_all_datasets(start_year=1999, end_year=2022)\n",
    "datasets_tot.keys()  # Display the keys of the dictionary\n",
    "#TAKES around 1.30 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_orig_with_perf(orig_data, perf_data):\n",
    "    merged_data = pd.merge(perf_data, orig_data, on=\"LSN\", how=\"left\")\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged 1999\n",
      "merged 2000\n",
      "merged 2001\n",
      "merged 2002\n",
      "merged 2003\n",
      "merged 2004\n",
      "merged 2005\n",
      "merged 2006\n",
      "merged 2007\n",
      "merged 2008\n",
      "merged 2009\n",
      "merged 2010\n",
      "merged 2011\n",
      "merged 2012\n",
      "merged 2013\n",
      "merged 2014\n",
      "merged 2015\n",
      "merged 2016\n",
      "merged 2017\n",
      "merged 2018\n",
      "merged 2019\n",
      "merged 2020\n",
      "merged 2021\n",
      "merged 2022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['fm_1999', 'fm_2000', 'fm_2001', 'fm_2002', 'fm_2003', 'fm_2004', 'fm_2005', 'fm_2006', 'fm_2007', 'fm_2008', 'fm_2009', 'fm_2010', 'fm_2011', 'fm_2012', 'fm_2013', 'fm_2014', 'fm_2015', 'fm_2016', 'fm_2017', 'fm_2018', 'fm_2019', 'fm_2020', 'fm_2021', 'fm_2022'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_all_datasets(datasets):\n",
    "    \"\"\"\n",
    "    Merge all origination and performance datasets within the provided dictionary according to their year.\n",
    "    \n",
    "    Parameters:\n",
    "    - datasets: Dictionary containing formatted origination and performance datasets.\n",
    "    \n",
    "    Returns:\n",
    "    - merged_datasets: Dictionary containing merged datasets for each year.\n",
    "    \"\"\"\n",
    "    merged_datasets = {}\n",
    "    # Extract the range of years from the dataset keys\n",
    "    years = sorted(set(int(key.split(\"_\")[-1]) for key in datasets.keys()))\n",
    "    for year in years:\n",
    "        orig_key = f\"orig_{year}\"\n",
    "        perf_key = f\"perf_{year}\"\n",
    "        if orig_key in datasets and perf_key in datasets:\n",
    "            merged_data = merge_orig_with_perf(datasets[orig_key], datasets[perf_key])\n",
    "            merged_data['Date'] = pd.to_datetime(merged_data['MRP'], format = '%Y%m')\n",
    "            merged_data = merged_data.drop(['MRP'], axis=1)\n",
    "            merged_data = merged_data[[\"Date\"] + [\"LSN\"] + [col for col in merged_data.columns if col != \"LSN\" and col != \"Date\"]]\n",
    "            merged_datasets[f\"fm_{year}\"] = merged_data\n",
    "            print(\"merged\", year)\n",
    "    return merged_datasets\n",
    "\n",
    "# Merge all datasets in the provided dictionary (in this case, datasets_demo)\n",
    "merged_datasets = merge_all_datasets(datasets_tot)\n",
    "merged_datasets.keys()  # Display the keys of the merged datasets dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>LSN</th>\n",
       "      <th>CLDS</th>\n",
       "      <th>CIR</th>\n",
       "      <th>ELTV</th>\n",
       "      <th>DDD</th>\n",
       "      <th>CS</th>\n",
       "      <th>FPD</th>\n",
       "      <th>FIRST_F</th>\n",
       "      <th>MD</th>\n",
       "      <th>CLTV</th>\n",
       "      <th>DTI</th>\n",
       "      <th>LTV</th>\n",
       "      <th>OIR</th>\n",
       "      <th>P_TYPE</th>\n",
       "      <th>POSTAL</th>\n",
       "      <th>OLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-09-01</td>\n",
       "      <td>F99Q10000029</td>\n",
       "      <td>0</td>\n",
       "      <td>6.375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618</td>\n",
       "      <td>200210</td>\n",
       "      <td>N</td>\n",
       "      <td>202902</td>\n",
       "      <td>85</td>\n",
       "      <td>24</td>\n",
       "      <td>85</td>\n",
       "      <td>6.375</td>\n",
       "      <td>SF</td>\n",
       "      <td>44200</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-10-01</td>\n",
       "      <td>F99Q10000029</td>\n",
       "      <td>0</td>\n",
       "      <td>6.375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618</td>\n",
       "      <td>200210</td>\n",
       "      <td>N</td>\n",
       "      <td>202902</td>\n",
       "      <td>85</td>\n",
       "      <td>24</td>\n",
       "      <td>85</td>\n",
       "      <td>6.375</td>\n",
       "      <td>SF</td>\n",
       "      <td>44200</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>F99Q10000029</td>\n",
       "      <td>0</td>\n",
       "      <td>6.375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618</td>\n",
       "      <td>200210</td>\n",
       "      <td>N</td>\n",
       "      <td>202902</td>\n",
       "      <td>85</td>\n",
       "      <td>24</td>\n",
       "      <td>85</td>\n",
       "      <td>6.375</td>\n",
       "      <td>SF</td>\n",
       "      <td>44200</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-12-01</td>\n",
       "      <td>F99Q10000029</td>\n",
       "      <td>0</td>\n",
       "      <td>6.375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618</td>\n",
       "      <td>200210</td>\n",
       "      <td>N</td>\n",
       "      <td>202902</td>\n",
       "      <td>85</td>\n",
       "      <td>24</td>\n",
       "      <td>85</td>\n",
       "      <td>6.375</td>\n",
       "      <td>SF</td>\n",
       "      <td>44200</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>F99Q10000029</td>\n",
       "      <td>0</td>\n",
       "      <td>6.375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618</td>\n",
       "      <td>200210</td>\n",
       "      <td>N</td>\n",
       "      <td>202902</td>\n",
       "      <td>85</td>\n",
       "      <td>24</td>\n",
       "      <td>85</td>\n",
       "      <td>6.375</td>\n",
       "      <td>SF</td>\n",
       "      <td>44200</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date           LSN CLDS    CIR  ELTV  DDD   CS     FPD FIRST_F   \n",
       "0 2002-09-01  F99Q10000029    0  6.375   NaN  NaN  618  200210       N  \\\n",
       "1 2002-10-01  F99Q10000029    0  6.375   NaN  NaN  618  200210       N   \n",
       "2 2002-11-01  F99Q10000029    0  6.375   NaN  NaN  618  200210       N   \n",
       "3 2002-12-01  F99Q10000029    0  6.375   NaN  NaN  618  200210       N   \n",
       "4 2003-01-01  F99Q10000029    0  6.375   NaN  NaN  618  200210       N   \n",
       "\n",
       "       MD  CLTV  DTI  LTV    OIR P_TYPE  POSTAL  OLT  \n",
       "0  202902    85   24   85  6.375     SF   44200  317  \n",
       "1  202902    85   24   85  6.375     SF   44200  317  \n",
       "2  202902    85   24   85  6.375     SF   44200  317  \n",
       "3  202902    85   24   85  6.375     SF   44200  317  \n",
       "4  202902    85   24   85  6.375     SF   44200  317  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_1999 = merged_datasets['fm_1999']\n",
    "fm_1999.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datasets_tot\n",
    "del cols_keep_orig\n",
    "del cols_keep_perf\n",
    "del layout\n",
    "del orig_column_names\n",
    "del orig_data_types\n",
    "del perf_column_names\n",
    "del perf_data_types\n",
    "del perf_layout\n",
    "del orig_layout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Columns and add 3ZiP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in merged_datasets.keys():\n",
    "#     merged_datasets[key]['Date'] = merged_datasets[key]['MRP'].astype(str)\n",
    "#     merged_datasets[key]['3ZIP'] = merged_datasets[key]['POSTAL'].astype(str).str[:3].astype('int16')\n",
    "#     #Transform DDD to 0 if NaN and 1 if Y\n",
    "#     merged_datasets[key]['DDD'] = merged_datasets[key]['DDD'].fillna(0)\n",
    "#     merged_datasets[key]['DDD'] = merged_datasets[key]['DDD'].replace('Y', 1)\n",
    "#     #Transform FIRST_F to 0 if N and 1 if Y\n",
    "#     merged_datasets[key]['FIRST_F'] = merged_datasets[key]['FIRST_F'].replace('N', 0)\n",
    "#     merged_datasets[key]['FIRST_F'] = merged_datasets[key]['FIRST_F'].replace('Y', 1)\n",
    "#     #Convert int64 to int32 or int16 or bool\n",
    "#     merged_datasets[key]['DDD'] = merged_datasets[key]['DDD'].astype('bool')\n",
    "#     merged_datasets[key]['FIRST_F'] = merged_datasets[key]['FIRST_F'].astype('bool')\n",
    "#     merged_datasets[key]['ELTV'] = merged_datasets[key]['ELTV'].astype('Int16')\n",
    "#     merged_datasets[key]['CS'] = merged_datasets[key]['CS'].astype('Int16')\n",
    "#     merged_datasets[key]['CLTV'] = merged_datasets[key]['CLTV'].astype('Int16')\n",
    "#     merged_datasets[key]['OLT'] = merged_datasets[key]['OLT'].astype('Int16')\n",
    "#     merged_datasets[key]['DTI'] = merged_datasets[key]['DTI'].astype('Int16')\n",
    "#     merged_datasets[key]['FPD'] = merged_datasets[key]['FPD'].astype('Int32')\n",
    "#     merged_datasets[key]['MD'] = merged_datasets[key]['MD'].astype('int32')\n",
    "#     #Drop POSTAL and MRP\n",
    "#     merged_datasets[key].drop(['POSTAL'], axis=1, inplace=True)\n",
    "#     merged_datasets[key].drop(['MRP'], axis=1, inplace=True)\n",
    "#     #Move Date and 3ZIP to the front\n",
    "#     merged_datasets[key] = merged_datasets[key][[\"Date\", \"3ZIP\"] + [col for col in merged_datasets[key].columns if col not in [\"Date\", \"3ZIP\"]]]\n",
    "#     print(f\"{key} added and dropped\")\n",
    "# merged_datasets['fm_2022'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 29.05 ss\n",
      "fm_2022 added and dropped\n",
      "[########################################] | 100% Completed | 29.53 ss\n",
      "fm_2021 added and dropped\n",
      "[########################################] | 100% Completed | 30.73 ss\n",
      "fm_2020 added and dropped\n",
      "[########################################] | 100% Completed | 31.57 s\n",
      "fm_2019 added and dropped\n",
      "[########################################] | 100% Completed | 32.48 s\n",
      "fm_2018 added and dropped\n",
      "[########################################] | 100% Completed | 33.86 s\n",
      "fm_2017 added and dropped\n",
      "[########################################] | 100% Completed | 34.91 s\n",
      "fm_2016 added and dropped\n",
      "[########################################] | 100% Completed | 36.56 s\n",
      "fm_2015 added and dropped\n",
      "[########################################] | 100% Completed | 34.63 s\n",
      "fm_2014 added and dropped\n",
      "[########################################] | 100% Completed | 36.34 s\n",
      "fm_2013 added and dropped\n",
      "[########################################] | 100% Completed | 37.23 s\n",
      "fm_2012 added and dropped\n",
      "[########################################] | 100% Completed | 36.07 s\n",
      "fm_2011 added and dropped\n",
      "[########################################] | 100% Completed | 36.53 s\n",
      "fm_2010 added and dropped\n",
      "[########################################] | 100% Completed | 36.06 s\n",
      "fm_2009 added and dropped\n",
      "[########################################] | 100% Completed | 37.39 s\n",
      "fm_2008 added and dropped\n",
      "[########################################] | 100% Completed | 39.51 s\n",
      "fm_2007 added and dropped\n",
      "[########################################] | 100% Completed | 40.05 s\n",
      "fm_2006 added and dropped\n",
      "[########################################] | 100% Completed | 40.61 s\n",
      "fm_2005 added and dropped\n",
      "[########################################] | 100% Completed | 38.76 s\n",
      "fm_2004 added and dropped\n",
      "[########################################] | 100% Completed | 38.83 s\n",
      "fm_2003 added and dropped\n",
      "[########################################] | 100% Completed | 34.53 s\n",
      "fm_2002 added and dropped\n",
      "[########################################] | 100% Completed | 33.83 s\n",
      "fm_2001 added and dropped\n",
      "[########################################] | 100% Completed | 32.99 s\n",
      "fm_2000 added and dropped\n",
      "[########################################] | 100% Completed | 34.38 s\n",
      "fm_1999 added and dropped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>3ZIP</th>\n",
       "      <th>LSN</th>\n",
       "      <th>CLDS</th>\n",
       "      <th>CIR</th>\n",
       "      <th>ELTV</th>\n",
       "      <th>DDD</th>\n",
       "      <th>CS</th>\n",
       "      <th>FPD</th>\n",
       "      <th>FIRST_F</th>\n",
       "      <th>MD</th>\n",
       "      <th>CLTV</th>\n",
       "      <th>DTI</th>\n",
       "      <th>LTV</th>\n",
       "      <th>OIR</th>\n",
       "      <th>P_TYPE</th>\n",
       "      <th>OLT</th>\n",
       "      <th>D90</th>\n",
       "      <th>D180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>125</td>\n",
       "      <td>F22Q10000012</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>768</td>\n",
       "      <td>202203</td>\n",
       "      <td>0</td>\n",
       "      <td>203702</td>\n",
       "      <td>57</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>2.625</td>\n",
       "      <td>SF</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date 3ZIP           LSN  CLDS    CIR  ELTV  DDD   CS     FPD  FIRST_F   \n",
       "0 2022-02-01  125  F22Q10000012     0  2.625    57    0  768  202203        0  \\\n",
       "1 2022-03-01  125  F22Q10000012     0  2.625    48    0  768  202203        0   \n",
       "2 2022-04-01  125  F22Q10000012     0  2.625    52    0  768  202203        0   \n",
       "3 2022-05-01  125  F22Q10000012     0  2.625    40    0  768  202203        0   \n",
       "4 2022-06-01  125  F22Q10000012     0  2.625    39    0  768  202203        0   \n",
       "\n",
       "       MD  CLTV  DTI  LTV    OIR P_TYPE  OLT  D90  D180  \n",
       "0  203702    57   28   57  2.625     SF  180    0     0  \n",
       "1  203702    57   28   57  2.625     SF  180    0     0  \n",
       "2  203702    57   28   57  2.625     SF  180    0     0  \n",
       "3  203702    57   28   57  2.625     SF  180    0     0  \n",
       "4  203702    57   28   57  2.625     SF  180    0     0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics.progress import ProgressBar\n",
    "def process_dataset(df):\n",
    "    # Transformations\n",
    "    df['3ZIP'] = df['POSTAL'].astype(str).str[:3]\n",
    "    df['DDD'] = df['DDD'].fillna(0).replace('Y', 1)\n",
    "    df['FIRST_F'] = df['FIRST_F'].replace({'N': 0, 'Y': 1})\n",
    "    #change RA values to 99 in CLDS\n",
    "    df['CLDS'] = df['CLDS'].replace('RA', 99)\n",
    "    df['CLDS'] = df['CLDS'].astype('int16')\n",
    "    # Convert data types\n",
    "    # int16_cols = ['ELTV', 'CS', 'CLTV', 'OLT', 'DTI']\n",
    "    # int32_cols = ['FPD', 'MD']\n",
    "    # for col in int16_cols:\n",
    "    #     df[col] = df[col].astype('Int16')\n",
    "    # for col in int32_cols:\n",
    "    #     df[col] = df[col].astype('Int32')\n",
    "    df['D90'] = 0\n",
    "    df['D180'] = 0\n",
    "    # Drop columns\n",
    "    df = df.drop(['POSTAL'], axis=1)\n",
    "    \n",
    "    # Process group\n",
    "    def process_group(group):\n",
    "        for val, offset, column in [(3, 3, 'D90'), (7, 6, 'D180')]:\n",
    "            if val in group['CLDS'].values:\n",
    "                delinquency_date = group[group['CLDS'] == val]['Date'].min()\n",
    "                back_date = delinquency_date - pd.DateOffset(months=offset)\n",
    "                group.loc[group['Date'] == back_date, column] = 1\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby('LSN').apply(process_group, meta=df._meta)\n",
    "    #ungroup df\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Move Date and 3ZIP to the front\n",
    "    df = df[[\"Date\", \"3ZIP\"] + [col for col in df.columns if col not in [\"Date\", \"3ZIP\"]]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert to Dask DataFrames and process\n",
    "with ProgressBar():\n",
    "    for key in merged_datasets.keys():\n",
    "        ddf = dd.from_pandas(merged_datasets[key], npartitions=6)\n",
    "        merged_datasets[key] = process_dataset(ddf).compute()\n",
    "        print(f\"{key} added and dropped\")\n",
    "\n",
    "merged_datasets['fm_2022'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D90\n",
       "0    2409669\n",
       "1       4655\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_datasets['fm_2008']['D90'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_datasets['fm_2022'].dtypes)\n",
    "display(merged_datasets['fm_2022']['DTI'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/mortgage_data/fm_datasets.pickle\", \"wb\") as f:\n",
    "    pickle.dump(merged_datasets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here you can work with fm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../Data/mortgage_data/fm_datasets.pickle\", \"rb\") as f:\n",
    "    fm_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query that binds fm_2022 and fm_2021\n",
    "query = \"SELECT * FROM fm_2022 UNION ALL SELECT * FROM fm_2021;\"\n",
    "\n",
    "fm_21_22 = pd.read_sql_query(query, conn)\n",
    "#save to csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge ENSO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load enso_mei_long.csv\n",
    "enso_mei_long = pd.read_csv(\"../Data/enso_mei_long.csv\")\n",
    "#Transform Month Dec to 12, Jan to 1, Feb to 2, etc.\n",
    "enso_mei_long['Month'] = enso_mei_long['Month'].apply(extract_month)\n",
    "enso_mei_long['Date'] = enso_mei_long.apply(lambda row: f\"{row['Year'].astype(int)}{row['Month'].astype(int):02}\", axis = 1)\n",
    "enso_mei_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_mei_long = enso_mei_long[['Date', 'MEI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_mei_long.to_sql('enso_mei_long', conn, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add HURR data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hrcn_data = pd.read_csv('mainland_usa_gdf_HRCN.csv', dtype={'3ZIP':str})\n",
    "hrcn_data_short = hrcn_data[['3ZIP', 'HRCN_RISKS', 'HRCN_RISKV', 'HRCN_EVNTS', 'HRCN_EALS']]\n",
    "hrcn_data_short.to_sql('hrcn_data_short', conn, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate HRCN data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_hrcn = f\"\"\" \n",
    "SELECT * \n",
    "FROM hrcn_data_short \n",
    "GROUP BY \"3ZIP\"\n",
    "\"\"\"\n",
    "hrcn_data_short_agg_1 = pd.read_sql_query(query_hrcn, conn)\n",
    "hrcn_data_short_agg_1.to_sql('hrcn_data_short_agg_1', conn, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to merge zip data to performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_zip_data(perf_data, zip_data, perf_zip_col, zip_data_col):\n",
    "    \"\"\"\n",
    "    Merge external data with the performance dataset based on the 3zip code.\n",
    "    \n",
    "    Parameters:\n",
    "    - perf_data: The performance dataset.\n",
    "    - zip_data: The external dataset with 3zip level information.\n",
    "    - perf_zip_col: The zip column name in the performance dataset (might be 5-digit zip).\n",
    "    - zip_data_col: The 3zip column name in the external dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - Merged dataset.\n",
    "    \"\"\"\n",
    "    # Convert 5-digit zip code to 3zip format\n",
    "    perf_data['3zip'] = perf_data[perf_zip_col].astype(str).str[:3]\n",
    "    zip_data['3zip'] = zip_data[zip_data_col].astype(str).str[:3]\n",
    "    \n",
    "    # Merge datasets based on the 3zip code\n",
    "    merged_data = pd.merge(perf_data, zip_data, left_on='3zip', right_on='3zip', how='left')\n",
    "    \n",
    "    return merged_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
